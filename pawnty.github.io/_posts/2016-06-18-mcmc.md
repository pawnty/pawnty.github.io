---
layout: post
title: 马尔可夫链蒙特卡洛
description: "介绍McMC采样算法"
modified: 2016-06-18
tags: [draft]
categories: [MachineLearning]
---

## 马尔可夫链和平稳分布
周末，笨小孩妈妈出门去，只留笨小孩一个人在家做作业。笨小孩想看电视，但是被妈妈发现后会挨揍。所以，笨小孩每看1分钟电视或做1分钟作业就要随机地决定是继续当前活动还是换到另外一个活动：如果当时正在看电视，继续看电视的概率为$P_{tt}$，换为做作业的概率为$1-P_{tt}$；如果当时在做作业，继续做作业的概率为$P_{hh}$，换为看电视的概率为$1-P_{hh}$。问题来了：笨小孩挨揍的概率是多少？

这个问题里，设笨小孩在第$i$分钟的活动为$X_i$。笨小孩下次活动的概率只与当前的活动有关，只要当前的活动是做作业，不管之前做了多少作业、看了多少电视，下一步做作业的概率都是$P_{hh}$，看电视的概率都是$1-P_{hh}$；只要当前的活动是做作业，不管之前做了多少作业、看了多少电视，下一步做作业的概率都是$P_{hh}$，看电视的概率都是$1-P_{hh}$。这种下一个状态只由当前状态决定的性质$P(X_{i+1}\vert X_0,X_1,\cdots,X_i)=P(X_{i+1}\vert X_i)$，称为马尔可夫性，满足马尔可夫性的随机序列$X_0, X_1, \cdots, X_n$称为马尔可夫链。

笨小孩是否挨揍取决于他妈妈回来时他是否在看电视。设妈妈在第$n$分钟回来，笨小孩挨揍的概率为也就是在$n$分钟看电视的概率$P(X_n=t)$。

在第$i$分钟，笨小孩的活动服从$P(X_i)$，那么在第$i+1$分钟，笨小孩活动的概率为$$P(X_{i+1})=\sum_{X_i\in\{t, h\}}P(X_i)P(X_{i+1}\vert X_i).$$

上式是一个递推公式。只要知道初始状态的概率$P(X_0)$和各个活动间转换的概率，就可以求出第$n$分钟活动的概率$P(X_n)$。

来尝试一下，设$P_{tt}=0.1, P_{hh}=0.3$，计算出笨小孩分别以$1.0, 0.5, 0.3, 0$为初始看电视概率在各个时间挨揍的概率，结果如下图所示。

![enter image description here](http://pawnty.github.io/images/posts/mcmc/figure1.png)

从图中可以看到，不管笨小孩初始看电视的概率为多少，经过一段足够长的时间，笨小孩最后挨揍的概率都一样，并且不再变化，即最初看电视的概率对后面挨揍概率的影响逐渐减小直至完全消失。一段时间后挨揍的概率是否受活动转换概率的影响呢？再设$P_{tt}=0.2, P_{hh}=0.6$，重新绘制结果：

![enter image description here](http://pawnty.github.io/images/posts/mcmc/figure2.png)

一段时间后，不管初始看电视概率为何，笨小孩挨揍的概率又变得一样，只不过这次与上次概率不同。

从这两个结果可以发现一个令人有点惊奇的规律：时间足够长的情况下，计算笨小孩挨揍的概率仅仅需要知道他两个活动间转换的概率，而初始活动的概率甚至确切的时间都可以忽略！

在马尔可夫过程中，最终状态所服从的不再随时间变化的概率分布称为该过程的平稳分布，记为

$$
\pi(X)=\lim_{n\rightarrow \infty}P(X_n).
$$

对于离散状态空间的马尔可夫过程，平稳分布应满足

$$
\pi(X_j)=\sum_i \pi(X_i)T_{ij}
$$

$T_{ij}$表示从状态$i$转到状态$j$的概率，矩阵$T=\{T_{ij}\}$称为转移矩阵，满足

$$T_{ij} \ge 0\\
\sum_j T_{ij} = 1.
$$

对于连续状态空间的马尔可夫过程，平稳分布应满足

$$
\pi(x)=\int_{x'} \pi(x')p(x', x)dx'
$$

其中$p(x', x)$叫做马尔可夫核(Markov Kernel)，须满足

$$p(x', x) \ge 0\\
\int_x p(x', x) dx = 1$$




